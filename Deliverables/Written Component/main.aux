\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{somvanshi2016review}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{3}{Introduction}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Definitions of terms used throughout this paper.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:Definitions}{{1}{4}{Definitions of terms used throughout this paper}{table.1}{}}
\citation{rojas1996backpropagation}
\citation{rumelhart1986learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A flowchart depicting a single-layer Neural Network. The input layer (blue) contains a single node, indicating a single value would be taken as an input. The hidden state (green) contains a single layer of \textit  {n} nodes. The output layer (red) contains a single node, indicating a single value would be received as an output. Graphic made with LucidChart.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:NueralNetwork}{{1}{5}{A flowchart depicting a single-layer Neural Network. The input layer (blue) contains a single node, indicating a single value would be taken as an input. The hidden state (green) contains a single layer of \textit {n} nodes. The output layer (red) contains a single node, indicating a single value would be received as an output. Graphic made with LucidChart}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Recurrent Neural Networks}{5}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A comparison of common activation functions. Note how the range of the inputs compares to the range of the outputs for different activations.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:Activations}{{2}{6}{A comparison of common activation functions. Note how the range of the inputs compares to the range of the outputs for different activations}{figure.2}{}}
\citation{hochreiter1997long}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A visualization of an `unrolled' recurrent neural network. This specific example uses 3 time-points of information to get a prediction.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:RNN}{{3}{7}{A visualization of an `unrolled' recurrent neural network. This specific example uses 3 time-points of information to get a prediction}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Long Short-Term Memory Networks}{8}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A visualization of long short-term memory networks.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:LSTM}{{4}{8}{A visualization of long short-term memory networks}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A visualization of the memory cells in LSTMs.}}{8}{figure.5}\protected@file@percent }
\newlabel{fig:MemoryCells}{{5}{8}{A visualization of the memory cells in LSTMs}{figure.5}{}}
\citation{understandinglstm}
\citation{lepot2017interpolation}
\citation{kalmanfilter}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A table of equations corresponding to each gate and node within Figure \ref {fig:MemoryCells}.}}{10}{table.2}\protected@file@percent }
\newlabel{tab:MemoryCellEquations}{{2}{10}{A table of equations corresponding to each gate and node within Figure \ref {fig:MemoryCells}}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}EVER Data}{10}{subsection.1.3}\protected@file@percent }
\newlabel{sec:EVER}{{1.3}{10}{EVER Data}{subsection.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A line plot of the time series used to train the LSTM in this project. Red vertical bars indicate a date with a missing value.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:P33}{{6}{11}{A line plot of the time series used to train the LSTM in this project. Red vertical bars indicate a date with a missing value}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A line plot of the time series used to train the LSTM in this project. Red lines indicate the imputed values using the Kalman filter.}}{11}{figure.7}\protected@file@percent }
\newlabel{fig:P33_Interpolated}{{7}{11}{A line plot of the time series used to train the LSTM in this project. Red lines indicate the imputed values using the Kalman filter}{figure.7}{}}
\citation{chollet2015keras}
\citation{lstmkeras}
\citation{cuda}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{12}{section.2}\protected@file@percent }
\newlabel{sec:methods}{{2}{12}{Methods}{section.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces A table depicting the architecture of the LSTM trained on the EVER time series data.}}{13}{table.3}\protected@file@percent }
\newlabel{fig:Architecture}{{3}{13}{A table depicting the architecture of the LSTM trained on the EVER time series data}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A series of plots obtained from the training process. Each plot shows the predicted water depths 31 days past the training set. The red line indicates the predictions from the LSTM and the blue line indicates the observed water depth. The number below each plot indicates how many generations the LSTM was trained for when the predictions were made.}}{14}{figure.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {2,530}}}{14}{subfigure.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {4,530}}}{14}{subfigure.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {6,530}}}{14}{subfigure.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {8,530}}}{14}{subfigure.8.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {10,530}}}{14}{subfigure.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {12,530}}}{14}{subfigure.8.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {14,530}}}{14}{subfigure.8.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {16,530}}}{14}{subfigure.8.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {18,530}}}{14}{subfigure.8.9}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {20,5530}}}{14}{subfigure.8.10}\protected@file@percent }
\newlabel{fig:Training}{{8}{14}{A series of plots obtained from the training process. Each plot shows the predicted water depths 31 days past the training set. The red line indicates the predictions from the LSTM and the blue line indicates the observed water depth. The number below each plot indicates how many generations the LSTM was trained for when the predictions were made}{figure.8}{}}
\citation{statsmodels}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A line plot comparing the 1 month out-of-sample predictions for 5 models initialized with the same framework. Each model was trained on the same training data for the number of iterations displayed below the plot.}}{15}{figure.9}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {2,000}}}{15}{subfigure.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {4,000}}}{15}{subfigure.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {6,000}}}{15}{subfigure.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {8,000}}}{15}{subfigure.9.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {10,000}}}{15}{subfigure.9.5}\protected@file@percent }
\newlabel{fig:Variability}{{9}{15}{A line plot comparing the 1 month out-of-sample predictions for 5 models initialized with the same framework. Each model was trained on the same training data for the number of iterations displayed below the plot}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{15}{section.3}\protected@file@percent }
\newlabel{sec:results}{{3}{15}{Results}{section.3}{}}
\citation{cowpertwait2009introductory}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A comparison of 31 day forecasts between the 3 models. Each model was trained on the same data.}}{16}{figure.10}\protected@file@percent }
\newlabel{fig:Comparison}{{10}{16}{A comparison of 31 day forecasts between the 3 models. Each model was trained on the same data}{figure.10}{}}
\citation{snoek2012practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}ARIMA Simulation Results}{17}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces A table depicting the architecture of the LSTM trained on the simulated ARIMA data.}}{17}{table.4}\protected@file@percent }
\newlabel{tab:ARIMAArchitecture}{{4}{17}{A table depicting the architecture of the LSTM trained on the simulated ARIMA data}{table.4}{}}
